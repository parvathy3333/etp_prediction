{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGn6I5H3CL3AvpiqQOfNwe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parvathy3333/etp_prediction/blob/main/membrane%20fouling%20rate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwZXALoIP55Q"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "# Set page configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"Membrane Fouling Prediction Tool\",\n",
        "    page_icon=\"ðŸ§ª\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Page title and description\n",
        "st.title(\"Membrane Fouling Prediction and Uncertainty Analysis\")\n",
        "st.subheader(\"Wastewater Treatment Plant Case Study\")\n",
        "\n",
        "# Sidebar with navigation\n",
        "st.sidebar.title(\"Navigation\")\n",
        "page = st.sidebar.radio(\"Go to\", [\"Data Upload & Exploration\", \"Model Training\", \"Prediction\", \"Uncertainty Analysis\"])\n",
        "\n",
        "# Initialize session state variables if they don't exist\n",
        "if 'data' not in st.session_state:\n",
        "    st.session_state.data = None\n",
        "if 'model' not in st.session_state:\n",
        "    st.session_state.model = None\n",
        "if 'X_train' not in st.session_state:\n",
        "    st.session_state.X_train = None\n",
        "if 'X_test' not in st.session_state:\n",
        "    st.session_state.X_test = None\n",
        "if 'y_train' not in st.session_state:\n",
        "    st.session_state.y_train = None\n",
        "if 'y_test' not in st.session_state:\n",
        "    st.session_state.y_test = None\n",
        "if 'scaler' not in st.session_state:\n",
        "    st.session_state.scaler = None\n",
        "if 'feature_names' not in st.session_state:\n",
        "    st.session_state.feature_names = None\n",
        "\n",
        "# Data Upload & Exploration page\n",
        "if page == \"Data Upload & Exploration\":\n",
        "    st.header(\"Data Upload & Exploration\")\n",
        "\n",
        "    # Data upload section\n",
        "    st.subheader(\"Upload your wastewater treatment plant data\")\n",
        "    uploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\n",
        "\n",
        "    # Example data option\n",
        "    use_example_data = st.checkbox(\"Use example data instead\")\n",
        "\n",
        "    if use_example_data:\n",
        "        # Generate example data for demonstration\n",
        "        np.random.seed(42)\n",
        "        n_samples = 1000\n",
        "\n",
        "        # Define features that might affect membrane fouling\n",
        "        data = {\n",
        "            'pH': np.random.uniform(6.0, 8.5, n_samples),\n",
        "            'MLSS_mg_L': np.random.uniform(2000, 5000, n_samples),  # Mixed liquor suspended solids\n",
        "            'TMP_kPa': np.random.uniform(10, 50, n_samples),  # Transmembrane pressure\n",
        "            'Flux_LMH': np.random.uniform(10, 30, n_samples),  # Permeate flux\n",
        "            'Temperature_C': np.random.uniform(15, 30, n_samples),\n",
        "            'COD_mg_L': np.random.uniform(200, 800, n_samples),  # Chemical oxygen demand\n",
        "            'NH4_mg_L': np.random.uniform(10, 40, n_samples),  # Ammonium concentration\n",
        "            'DO_mg_L': np.random.uniform(1, 5, n_samples),  # Dissolved oxygen\n",
        "            'Conductivity_uS_cm': np.random.uniform(500, 2000, n_samples),\n",
        "        }\n",
        "\n",
        "        # Generate fouling rate based on features (simplified model)\n",
        "        fouling_rate = (\n",
        "            0.05 * data['TMP_kPa'] +\n",
        "            0.02 * data['Flux_LMH'] +\n",
        "            -0.1 * data['DO_mg_L'] +\n",
        "            0.001 * data['MLSS_mg_L'] +\n",
        "            0.005 * data['COD_mg_L'] +\n",
        "            np.random.normal(0, 0.5, n_samples)  # Add some noise\n",
        "        )\n",
        "\n",
        "        data['Fouling_Rate'] = fouling_rate\n",
        "        example_df = pd.DataFrame(data)\n",
        "        st.session_state.data = example_df\n",
        "\n",
        "    elif uploaded_file is not None:\n",
        "        # Read the uploaded file\n",
        "        try:\n",
        "            data = pd.read_csv(uploaded_file)\n",
        "            st.session_state.data = data\n",
        "            st.success(\"Data successfully loaded!\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error: {e}\")\n",
        "\n",
        "    # Display and explore the data if available\n",
        "    if st.session_state.data is not None:\n",
        "        st.subheader(\"Data Preview\")\n",
        "        st.dataframe(st.session_state.data.head())\n",
        "\n",
        "        st.subheader(\"Data Information\")\n",
        "        buffer = BytesIO()\n",
        "        st.session_state.data.info(buf=buffer)\n",
        "        s = buffer.getvalue().decode()\n",
        "        st.text(s)\n",
        "\n",
        "        st.subheader(\"Statistical Summary\")\n",
        "        st.dataframe(st.session_state.data.describe())\n",
        "\n",
        "        st.subheader(\"Data Visualization\")\n",
        "\n",
        "        # Correlation heatmap\n",
        "        st.write(\"#### Correlation Heatmap\")\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "        corr = st.session_state.data.corr()\n",
        "        sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", ax=ax)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Distribution of target variable\n",
        "        st.write(\"#### Distribution of Fouling Rate\")\n",
        "        if 'Fouling_Rate' in st.session_state.data.columns:\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            sns.histplot(st.session_state.data['Fouling_Rate'], kde=True, ax=ax)\n",
        "            st.pyplot(fig)\n",
        "        else:\n",
        "            st.warning(\"No 'Fouling_Rate' column found. Please ensure your data includes the target variable.\")\n",
        "\n",
        "        # Feature histograms\n",
        "        st.write(\"#### Feature Distributions\")\n",
        "        selected_features = st.multiselect(\n",
        "            \"Select features to visualize\",\n",
        "            options=st.session_state.data.columns.tolist(),\n",
        "            default=st.session_state.data.columns.tolist()[:3]\n",
        "        )\n",
        "\n",
        "        if selected_features:\n",
        "            fig, axes = plt.subplots(len(selected_features), 1, figsize=(10, 3*len(selected_features)))\n",
        "            if len(selected_features) == 1:\n",
        "                axes = [axes]\n",
        "\n",
        "            for i, feature in enumerate(selected_features):\n",
        "                sns.histplot(st.session_state.data[feature], kde=True, ax=axes[i])\n",
        "                axes[i].set_title(f\"Distribution of {feature}\")\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "        # Select target variable\n",
        "        st.subheader(\"Select Target Variable\")\n",
        "        target_col = st.selectbox(\n",
        "            \"Which column represents the membrane fouling rate?\",\n",
        "            options=st.session_state.data.columns.tolist(),\n",
        "            index=st.session_state.data.columns.tolist().index('Fouling_Rate') if 'Fouling_Rate' in st.session_state.data.columns else 0\n",
        "        )\n",
        "\n",
        "        # Select features to use\n",
        "        st.subheader(\"Select Features\")\n",
        "        feature_cols = st.multiselect(\n",
        "            \"Which columns would you like to use as features?\",\n",
        "            options=[col for col in st.session_state.data.columns if col != target_col],\n",
        "            default=[col for col in st.session_state.data.columns if col != target_col]\n",
        "        )\n",
        "\n",
        "        if st.button(\"Prepare Data for Model Training\"):\n",
        "            if feature_cols and target_col:\n",
        "                st.session_state.feature_names = feature_cols\n",
        "                st.session_state.target_name = target_col\n",
        "                st.success(\"Data prepared for model training! Please go to the Model Training page.\")\n",
        "            else:\n",
        "                st.error(\"Please select both features and target variable.\")\n",
        "\n",
        "\n",
        "# Model Training page\n",
        "elif page == \"Model Training\":\n",
        "    st.header(\"Model Training\")\n",
        "\n",
        "    if st.session_state.data is None:\n",
        "        st.warning(\"Please upload or generate data first on the Data Upload & Exploration page.\")\n",
        "    elif not hasattr(st.session_state, 'feature_names') or not st.session_state.feature_names:\n",
        "        st.warning(\"Please select features and target variable on the Data Upload & Exploration page.\")\n",
        "    else:\n",
        "        # Show selected features and target\n",
        "        st.subheader(\"Selected Features and Target\")\n",
        "        st.write(f\"Target Variable: {st.session_state.target_name}\")\n",
        "        st.write(f\"Selected Features: {', '.join(st.session_state.feature_names)}\")\n",
        "\n",
        "        # Model configuration\n",
        "        st.subheader(\"Model Configuration\")\n",
        "\n",
        "        col1, col2 = st.columns(2)\n",
        "\n",
        "        with col1:\n",
        "            test_size = st.slider(\"Test Set Size (%)\", min_value=10, max_value=50, value=20, step=5) / 100\n",
        "            n_estimators = st.slider(\"Number of Trees\", min_value=50, max_value=500, value=100, step=50)\n",
        "\n",
        "        with col2:\n",
        "            max_depth = st.slider(\"Maximum Tree Depth\", min_value=3, max_value=30, value=10, step=1)\n",
        "            random_state = st.slider(\"Random State\", min_value=0, max_value=100, value=42, step=1)\n",
        "\n",
        "        # Train model button\n",
        "        if st.button(\"Train Random Forest Model\"):\n",
        "            with st.spinner(\"Training model...\"):\n",
        "                try:\n",
        "                    # Prepare data\n",
        "                    X = st.session_state.data[st.session_state.feature_names]\n",
        "                    y = st.session_state.data[st.session_state.target_name]\n",
        "\n",
        "                    # Split data\n",
        "                    X_train, X_test, y_train, y_test = train_test_split(\n",
        "                        X, y, test_size=test_size, random_state=random_state\n",
        "                    )\n",
        "\n",
        "                    # Scale features\n",
        "                    scaler = StandardScaler()\n",
        "                    X_train_scaled = scaler.fit_transform(X_train)\n",
        "                    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "                    # Train Random Forest model\n",
        "                    model = RandomForestRegressor(\n",
        "                        n_estimators=n_estimators,\n",
        "                        max_depth=max_depth,\n",
        "                        random_state=random_state,\n",
        "                        n_jobs=-1\n",
        "                    )\n",
        "\n",
        "                    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "                    # Save the model and data\n",
        "                    st.session_state.model = model\n",
        "                    st.session_state.X_train = X_train\n",
        "                    st.session_state.X_test = X_test\n",
        "                    st.session_state.y_train = y_train\n",
        "                    st.session_state.y_test = y_test\n",
        "                    st.session_state.scaler = scaler\n",
        "\n",
        "                    # Make predictions\n",
        "                    y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "                    # Calculate metrics\n",
        "                    r2 = r2_score(y_test, y_pred)\n",
        "                    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "                    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "                    # Display metrics\n",
        "                    st.subheader(\"Model Performance Metrics\")\n",
        "\n",
        "                    metric_col1, metric_col2, metric_col3 = st.columns(3)\n",
        "\n",
        "                    with metric_col1:\n",
        "                        st.metric(\"RÂ² Score\", f\"{r2:.4f}\")\n",
        "\n",
        "                    with metric_col2:\n",
        "                        st.metric(\"RMSE\", f\"{rmse:.4f}\")\n",
        "\n",
        "                    with metric_col3:\n",
        "                        st.metric(\"MAE\", f\"{mae:.4f}\")\n",
        "\n",
        "                    # Feature importance\n",
        "                    st.subheader(\"Feature Importance\")\n",
        "                    importance = model.feature_importances_\n",
        "                    indices = np.argsort(importance)[::-1]\n",
        "\n",
        "                    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                    feature_names = np.array(st.session_state.feature_names)\n",
        "                    sns.barplot(x=importance[indices], y=feature_names[indices], ax=ax)\n",
        "                    ax.set_title(\"Feature Importance\")\n",
        "                    ax.set_xlabel(\"Importance\")\n",
        "                    ax.set_ylabel(\"Feature\")\n",
        "                    st.pyplot(fig)\n",
        "\n",
        "                    # Plot actual vs predicted values\n",
        "                    st.subheader(\"Actual vs. Predicted Values\")\n",
        "                    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                    ax.scatter(y_test, y_pred, alpha=0.5)\n",
        "                    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "                    ax.set_xlabel(\"Actual Values\")\n",
        "                    ax.set_ylabel(\"Predicted Values\")\n",
        "                    ax.set_title(\"Actual vs. Predicted Values\")\n",
        "                    st.pyplot(fig)\n",
        "\n",
        "                    # Save model button\n",
        "                    st.subheader(\"Save Model\")\n",
        "\n",
        "                    def get_model_download_link(model):\n",
        "                        output = BytesIO()\n",
        "                        pickle.dump(model, output)\n",
        "                        b64 = base64.b64encode(output.getvalue()).decode()\n",
        "                        return f'<a href=\"data:application/octet-stream;base64,{b64}\" download=\"membrane_fouling_model.pkl\">Download trained model</a>'\n",
        "\n",
        "                    st.markdown(get_model_download_link(model), unsafe_allow_html=True)\n",
        "\n",
        "                    st.success(\"Model trained successfully! You can now use it to make predictions or analyze uncertainty.\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error during model training: {e}\")\n",
        "\n",
        "# Prediction page\n",
        "elif page == \"Prediction\":\n",
        "    st.header(\"Membrane Fouling Prediction\")\n",
        "\n",
        "    if st.session_state.model is None:\n",
        "        st.warning(\"Please train a model first on the Model Training page.\")\n",
        "    else:\n",
        "        st.subheader(\"Enter Parameter Values for Prediction\")\n",
        "\n",
        "        # Create input fields for each feature\n",
        "        input_values = {}\n",
        "\n",
        "        # Create columns for better layout\n",
        "        col1, col2 = st.columns(2)\n",
        "\n",
        "        # Add input fields for all features\n",
        "        for i, feature in enumerate(st.session_state.feature_names):\n",
        "            # Get min and max values from training data\n",
        "            min_val = float(st.session_state.X_train[feature].min())\n",
        "            max_val = float(st.session_state.X_train[feature].max())\n",
        "            mean_val = float(st.session_state.X_train[feature].mean())\n",
        "\n",
        "            # Create input slider in appropriate column\n",
        "            if i % 2 == 0:\n",
        "                with col1:\n",
        "                    input_values[feature] = st.slider(\n",
        "                        f\"{feature}\",\n",
        "                        min_value=min_val,\n",
        "                        max_value=max_val,\n",
        "                        value=mean_val,\n",
        "                        step=(max_val - min_val) / 100\n",
        "                    )\n",
        "            else:\n",
        "                with col2:\n",
        "                    input_values[feature] = st.slider(\n",
        "                        f\"{feature}\",\n",
        "                        min_value=min_val,\n",
        "                        max_value=max_val,\n",
        "                        value=mean_val,\n",
        "                        step=(max_val - min_val) / 100\n",
        "                    )\n",
        "\n",
        "        # Add a button to make prediction\n",
        "        if st.button(\"Predict Fouling Rate\"):\n",
        "            with st.spinner(\"Making prediction...\"):\n",
        "                try:\n",
        "                    # Convert input to dataframe\n",
        "                    input_df = pd.DataFrame([input_values])\n",
        "\n",
        "                    # Scale input data\n",
        "                    input_scaled = st.session_state.scaler.transform(input_df)\n",
        "\n",
        "                    # Make prediction\n",
        "                    prediction = st.session_state.model.predict(input_scaled)\n",
        "\n",
        "                    # For uncertainty, get predictions from individual trees\n",
        "                    tree_preds = np.array([tree.predict(input_scaled)[0] for tree in st.session_state.model.estimators_])\n",
        "\n",
        "                    # Calculate uncertainty metrics\n",
        "                    mean_pred = np.mean(tree_preds)\n",
        "                    std_pred = np.std(tree_preds)\n",
        "                    conf_interval = (mean_pred - 1.96 * std_pred, mean_pred + 1.96 * std_pred)\n",
        "\n",
        "                    # Display prediction\n",
        "                    st.subheader(\"Prediction Results\")\n",
        "\n",
        "                    col1, col2, col3 = st.columns(3)\n",
        "\n",
        "                    with col1:\n",
        "                        st.metric(\"Predicted Fouling Rate\", f\"{prediction[0]:.4f}\")\n",
        "\n",
        "                    with col2:\n",
        "                        st.metric(\"Standard Deviation\", f\"{std_pred:.4f}\")\n",
        "\n",
        "                    with col3:\n",
        "                        st.metric(\"Coefficient of Variation\", f\"{(std_pred/mean_pred)*100:.2f}%\")\n",
        "\n",
        "                    st.write(f\"95% Confidence Interval: [{conf_interval[0]:.4f}, {conf_interval[1]:.4f}]\")\n",
        "\n",
        "                    # Visualize the prediction distribution\n",
        "                    st.subheader(\"Prediction Distribution from Random Forest Trees\")\n",
        "\n",
        "                    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                    sns.histplot(tree_preds, kde=True, ax=ax)\n",
        "                    ax.axvline(prediction[0], color='red', linestyle='--', label='Prediction')\n",
        "                    ax.axvline(conf_interval[0], color='green', linestyle='--', label='95% CI Lower')\n",
        "                    ax.axvline(conf_interval[1], color='green', linestyle='--', label='95% CI Upper')\n",
        "                    ax.set_xlabel(\"Predicted Fouling Rate\")\n",
        "                    ax.set_ylabel(\"Frequency\")\n",
        "                    ax.legend()\n",
        "                    st.pyplot(fig)\n",
        "\n",
        "                    # Contextual interpretation\n",
        "                    st.subheader(\"Interpretation\")\n",
        "\n",
        "                    if std_pred / mean_pred < 0.1:\n",
        "                        confidence = \"high\"\n",
        "                    elif std_pred / mean_pred < 0.2:\n",
        "                        confidence = \"moderate\"\n",
        "                    else:\n",
        "                        confidence = \"low\"\n",
        "\n",
        "                    st.write(f\"The model predicts a fouling rate of {prediction[0]:.4f} with {confidence} confidence.\")\n",
        "                    st.write(f\"The standard deviation of {std_pred:.4f} represents the model's uncertainty.\")\n",
        "\n",
        "                    # Display what-if scenarios\n",
        "                    st.subheader(\"What-If Analysis\")\n",
        "                    st.write(\"How would changing parameter values affect the fouling rate?\")\n",
        "\n",
        "                    # Get top 3 important features\n",
        "                    importance = st.session_state.model.feature_importances_\n",
        "                    top_features_idx = np.argsort(importance)[::-1][:3]\n",
        "                    top_features = [st.session_state.feature_names[i] for i in top_features_idx]\n",
        "\n",
        "                    selected_feature = st.selectbox(\n",
        "                        \"Select a parameter to vary:\",\n",
        "                        options=top_features\n",
        "                    )\n",
        "\n",
        "                    if selected_feature:\n",
        "                        # Create range of values for the selected feature\n",
        "                        min_val = float(st.session_state.X_train[selected_feature].min())\n",
        "                        max_val = float(st.session_state.X_train[selected_feature].max())\n",
        "\n",
        "                        # Generate predictions across the range\n",
        "                        test_values = np.linspace(min_val, max_val, 50)\n",
        "                        predictions = []\n",
        "                        lower_bounds = []\n",
        "                        upper_bounds = []\n",
        "\n",
        "                        for val in test_values:\n",
        "                            # Copy the input values and change only the selected feature\n",
        "                            test_input = input_values.copy()\n",
        "                            test_input[selected_feature] = val\n",
        "\n",
        "                            # Convert to DataFrame\n",
        "                            test_df = pd.DataFrame([test_input])\n",
        "\n",
        "                            # Scale\n",
        "                            test_scaled = st.session_state.scaler.transform(test_df)\n",
        "\n",
        "                            # Get predictions from all trees\n",
        "                            tree_preds = np.array([tree.predict(test_scaled)[0] for tree in st.session_state.model.estimators_])\n",
        "\n",
        "                            # Calculate mean and CI\n",
        "                            mean_pred = np.mean(tree_preds)\n",
        "                            std_pred = np.std(tree_preds)\n",
        "\n",
        "                            predictions.append(mean_pred)\n",
        "                            lower_bounds.append(mean_pred - 1.96 * std_pred)\n",
        "                            upper_bounds.append(mean_pred + 1.96 * std_pred)\n",
        "\n",
        "                        # Plot what-if analysis\n",
        "                        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                        ax.plot(test_values, predictions, 'b-', label='Predicted Fouling Rate')\n",
        "                        ax.fill_between(test_values, lower_bounds, upper_bounds, alpha=0.3, label='95% CI')\n",
        "                        ax.set_xlabel(selected_feature)\n",
        "                        ax.set_ylabel('Predicted Fouling Rate')\n",
        "                        ax.set_title(f'Effect of {selected_feature} on Fouling Rate')\n",
        "                        ax.axvline(input_values[selected_feature], color='red', linestyle='--', label='Current Value')\n",
        "                        ax.legend()\n",
        "                        st.pyplot(fig)\n",
        "\n",
        "                        # Interpretation of what-if analysis\n",
        "                        slope = (predictions[-1] - predictions[0]) / (test_values[-1] - test_values[0])\n",
        "                        if abs(slope) < 0.01:\n",
        "                            impact = \"minimal impact\"\n",
        "                        elif abs(slope) < 0.1:\n",
        "                            impact = \"moderate impact\"\n",
        "                        else:\n",
        "                            impact = \"significant impact\"\n",
        "\n",
        "                        direction = \"increase\" if slope > 0 else \"decrease\"\n",
        "\n",
        "                        st.write(f\"Changing {selected_feature} has a {impact} on the fouling rate.\")\n",
        "                        st.write(f\"As {selected_feature} increases, the fouling rate tends to {direction}.\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error during prediction: {e}\")\n",
        "\n",
        "# Uncertainty Analysis page\n",
        "elif page == \"Uncertainty Analysis\":\n",
        "    st.header(\"Uncertainty Analysis\")\n",
        "\n",
        "    if st.session_state.model is None:\n",
        "        st.warning(\"Please train a model first on the Model Training page.\")\n",
        "    else:\n",
        "        st.subheader(\"Model Uncertainty Analysis\")\n",
        "\n",
        "        # Generate predictions for test set\n",
        "        X_test_scaled = st.session_state.scaler.transform(st.session_state.X_test)\n",
        "        y_pred = st.session_state.model.predict(X_test_scaled)\n",
        "\n",
        "        # Get predictions from individual trees for uncertainty estimation\n",
        "        tree_preds = np.array([tree.predict(X_test_scaled) for tree in st.session_state.model.estimators_])\n",
        "\n",
        "        # Calculate standard deviation across trees for each test point\n",
        "        y_std = np.std(tree_preds, axis=0)\n",
        "\n",
        "        # Calculate confidence intervals\n",
        "        y_lower = y_pred - 1.96 * y_std\n",
        "        y_upper = y_pred + 1.96 * y_std\n",
        "\n",
        "        # Calculate if actual is within CI\n",
        "        within_ci = (st.session_state.y_test >= y_lower) & (st.session_state.y_test <= y_upper)\n",
        "        ci_coverage = np.mean(within_ci) * 100\n",
        "\n",
        "        # Display CI coverage\n",
        "        st.metric(\"95% CI Coverage\", f\"{ci_coverage:.2f}%\",\n",
        "                 delta=f\"{ci_coverage - 95:.2f}%\" if ci_coverage != 95 else None)\n",
        "\n",
        "        # Plot uncertainty vs. error\n",
        "        st.subheader(\"Uncertainty vs. Error Analysis\")\n",
        "\n",
        "        abs_error = np.abs(st.session_state.y_test.values - y_pred)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        ax.scatter(y_std, abs_error, alpha=0.5)\n",
        "        ax.set_xlabel(\"Prediction Standard Deviation (Uncertainty)\")\n",
        "        ax.set_ylabel(\"Absolute Prediction Error\")\n",
        "        ax.set_title(\"Relationship Between Model Uncertainty and Error\")\n",
        "\n",
        "        # Add correlation line\n",
        "        m, b = np.polyfit(y_std, abs_error, 1)\n",
        "        ax.plot(y_std, m*y_std + b, 'r-', label=f'Correlation: {np.corrcoef(y_std, abs_error)[0,1]:.2f}')\n",
        "        ax.legend()\n",
        "\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Comment on uncertainty-error relationship\n",
        "        corr = np.corrcoef(y_std, abs_error)[0,1]\n",
        "        if corr > 0.5:\n",
        "            st.write(\"There is a strong positive correlation between model uncertainty and prediction error, \"\n",
        "                    \"indicating that the model's uncertainty estimates are reliable predictors of actual error.\")\n",
        "        elif corr > 0.3:\n",
        "            st.write(\"There is a moderate positive correlation between model uncertainty and prediction error, \"\n",
        "                    \"suggesting that the model's uncertainty estimates provide some useful information about prediction quality.\")\n",
        "        else:\n",
        "            st.write(\"There is a weak correlation between model uncertainty and prediction error, \"\n",
        "                    \"indicating that the model's uncertainty estimates may not reliably predict actual error.\")\n",
        "\n",
        "        # Plot sorted test predictions with CI\n",
        "        st.subheader(\"Predictions with Confidence Intervals\")\n",
        "\n",
        "        # Sort by prediction value for better visualization\n",
        "        sorted_indices = np.argsort(y_pred)\n",
        "        y_test_sorted = st.session_state.y_test.values[sorted_indices]\n",
        "        y_pred_sorted = y_pred[sorted_indices]\n",
        "        y_lower_sorted = y_lower[sorted_indices]\n",
        "        y_upper_sorted = y_upper[sorted_indices]\n",
        "\n",
        "        # Plot only a subset of points for clarity\n",
        "        sample_size = min(100, len(y_pred))\n",
        "        step = len(y_pred) // sample_size\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        x_indices = np.arange(len(y_pred_sorted))[::step]\n",
        "\n",
        "        # Plot actual values\n",
        "        ax.scatter(x_indices, y_test_sorted[::step], alpha=0.6, color='green', label='Actual')\n",
        "\n",
        "        # Plot predictions with CI\n",
        "        ax.errorbar(x_indices, y_pred_sorted[::step],\n",
        "                   yerr=[(y_pred_sorted[::step] - y_lower_sorted[::step]),\n",
        "                         (y_upper_sorted[::step] - y_pred_sorted[::step])],\n",
        "                   fmt='o', alpha=0.6, color='blue', label='Predicted with 95% CI')\n",
        "\n",
        "        ax.set_xlabel(\"Sample Index\")\n",
        "        ax.set_ylabel(\"Fouling Rate\")\n",
        "        ax.set_title(\"Predictions with Confidence Intervals vs. Actual Values\")\n",
        "        ax.legend()\n",
        "\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Uncertainty distribution\n",
        "        st.subheader(\"Uncertainty Distribution\")\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        sns.histplot(y_std, kde=True, ax=ax)\n",
        "        ax.set_xlabel(\"Prediction Standard Deviation\")\n",
        "        ax.set_ylabel(\"Frequency\")\n",
        "        ax.set_title(\"Distribution of Model Uncertainty\")\n",
        "\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Calibration analysis\n",
        "        st.subheader(\"Uncertainty Calibration Analysis\")\n",
        "\n",
        "        # Create uncertainty bins\n",
        "        n_bins = 10\n",
        "        uncertainty_bins = np.linspace(y_std.min(), y_std.max(), n_bins + 1)\n",
        "        bin_indices = np.digitize(y_std, uncertainty_bins) - 1\n",
        "\n",
        "        bin_errors = []\n",
        "        bin_uncertainties = []\n",
        "\n",
        "        for i in range(n_bins):\n",
        "            mask = bin_indices == i\n",
        "            if np.sum(mask) > 0:\n",
        "                bin_errors.append(np.mean(abs_error[mask]))\n",
        "                bin_uncertainties.append(np.mean(y_std[mask]))\n",
        "\n",
        "        if bin_errors and bin_uncertainties:\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            ax.plot(bin_uncertainties, bin_errors, 'o-')\n",
        "            ax.plot([0, max(bin_uncertainties)], [0, max(bin_uncertainties)], 'r--', label='Ideal Calibration')\n",
        "            ax.set_xlabel(\"Mean Predicted Uncertainty\")\n",
        "            ax.set_ylabel(\"Mean Observed Error\")\n",
        "            ax.set_title(\"Uncertainty Calibration Plot\")\n",
        "            ax.legend()\n",
        "\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            # Interpret calibration plot\n",
        "            ratio = np.mean(bin_errors) / np.mean(bin_uncertainties) if np.mean(bin_uncertainties) > 0 else 0\n",
        "\n",
        "            if 0.8 <= ratio <= 1.2:\n",
        "                st.write(\"The model's uncertainty estimates are well-calibrated. The predicted uncertainty levels align closely with observed errors.\")\n",
        "            elif ratio < 0.8:\n",
        "                st.write(\"The model tends to overestimate uncertainty. Actual errors are generally smaller than the model's uncertainty estimates.\")\n",
        "            else:\n",
        "                st.write(\"The model tends to underestimate uncertainty. Actual errors are generally larger than the model's uncertainty estimates.\")\n",
        "\n",
        "        # Risk assessment\n",
        "        st.subheader(\"Risk Assessment for Decision Making\")\n",
        "\n",
        "        # Calculate risk metrics\n",
        "        high_uncertainty_threshold = np.percentile(y_std, 75)\n",
        "        high_uncertainty_mask = y_std > high_uncertainty_threshold\n",
        "\n",
        "        high_error_threshold = np.percentile(abs_error, 75)\n",
        "        high_error_mask = abs_error > high_error_threshold\n",
        "\n",
        "        true_positive_rate = np.sum(high_uncertainty_mask & high_error_mask) / np.sum(high_error_mask) if np.sum(high_error_mask) > 0 else 0\n",
        "\n",
        "        st.write(f\"When the model shows high uncertainty (top 25%), it correctly identifies {true_positive_rate*100:.1f}% of high-error predictions.\")\n",
        "\n",
        "        if true_positive_rate > 0.7:\n",
        "            st.write(\"This suggests the uncertainty estimates are reliable indicators of prediction quality.\")\n",
        "        elif true_positive_rate > 0.5:\n",
        "            st.write(\"The uncertainty estimates provide moderate value for identifying potential prediction failures.\")\n",
        "        else:\n",
        "            st.write(\"The uncertainty estimates may not be reliable indicators of when the model will make large errors.\")\n",
        "\n",
        "        # Feature uncertainty contribution\n",
        "        st.subheader(\"Feature Contribution to Uncertainty\")\n",
        "\n",
        "        # Create a simple linear model to assess feature contribution to uncertainty\n",
        "        from sklearn.linear_model import LinearRegression\n",
        "\n",
        "        lr_model = LinearRegression()\n",
        "        lr_model.fit(st.session_state.X_test, y_std)\n",
        "\n",
        "        # Get coefficients\n",
        "        feature_uncertainty_coef = {feature: coef for feature, coef in zip(st.session_state.feature\n",
        "                                                                           # Get coefficients\n",
        "        feature_uncertainty_coef = {feature: coef for feature, coef in zip(st.session_state.feature_names, lr_model.coef_)}\n",
        "\n",
        "        # Sort by absolute coefficient value\n",
        "        sorted_uncertainty_features = sorted(feature_uncertainty_coef.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "        # Plot feature contributions to uncertainty\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        features = [x[0] for x in sorted_uncertainty_features]\n",
        "        coefs = [x[1] for x in sorted_uncertainty_features]\n",
        "\n",
        "        colors = ['red' if c < 0 else 'blue' for c in coefs]\n",
        "        sns.barplot(x=np.abs(coefs), y=features, palette=colors, ax=ax)\n",
        "        ax.set_xlabel(\"Absolute Coefficient Value\")\n",
        "        ax.set_ylabel(\"Feature\")\n",
        "        ax.set_title(\"Feature Contribution to Prediction Uncertainty\")\n",
        "\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Interpret feature uncertainty\n",
        "        st.write(\"Features with higher values contribute more to model uncertainty.\")\n",
        "        st.write(f\"The top uncertainty-contributing features are: {', '.join(features[:3])}\")\n",
        "\n",
        "        # Recommendations for reducing uncertainty\n",
        "        st.subheader(\"Recommendations for Reducing Prediction Uncertainty\")\n",
        "\n",
        "        st.write(\"Based on the uncertainty analysis, consider the following recommendations:\")\n",
        "        st.write(\"1. Collect more data for operating conditions that show high uncertainty\")\n",
        "        st.write(f\"2. Improve measurement precision for {', '.join(features[:3])}\")\n",
        "        st.write(\"3. Consider adding additional sensors or measurements that might better explain fouling behavior\")\n",
        "        st.write(\"4. For critical operational decisions, use the confidence intervals rather than point predictions\")\n",
        "\n",
        "        # Download uncertainty analysis report\n",
        "        st.subheader(\"Download Analysis Report\")\n",
        "\n",
        "        def get_uncertainty_report():\n",
        "            report = f\"\"\"\n",
        "            # Membrane Fouling Prediction Uncertainty Report\n",
        "\n",
        "            ## Model Performance\n",
        "            - 95% Confidence Interval Coverage: {ci_coverage:.2f}%\n",
        "            - Correlation between uncertainty and error: {corr:.2f}\n",
        "\n",
        "            ## Key Uncertainty Findings\n",
        "            - Top uncertainty-contributing features: {', '.join(features[:3])}\n",
        "            - Uncertainty detection rate for high errors: {true_positive_rate*100:.1f}%\n",
        "\n",
        "            ## Recommendations\n",
        "            1. Collect more data for operating conditions that show high uncertainty\n",
        "            2. Improve measurement precision for {', '.join(features[:3])}\n",
        "            3. Consider adding additional sensors or measurements that might better explain fouling behavior\n",
        "            4. For critical operational decisions, use the confidence intervals rather than point predictions\n",
        "            \"\"\"\n",
        "            return report\n",
        "\n",
        "        report_text = get_uncertainty_report()\n",
        "        report_bytes = report_text.encode()\n",
        "        b64 = base64.b64encode(report_bytes).decode()\n",
        "\n",
        "        st.markdown(\n",
        "            f'<a href=\"data:application/octet-stream;base64,{b64}\" download=\"uncertainty_analysis_report.md\">Download Uncertainty Analysis Report</a>',\n",
        "            unsafe_allow_html=True\n",
        "        )\n",
        "\n",
        "# Instructions for Google Colab setup\n",
        "st.sidebar.header(\"Setup Instructions\")\n",
        "\n",
        "if st.sidebar.checkbox(\"Show Setup Instructions\"):\n",
        "    st.sidebar.subheader(\"Google Colab Setup\")\n",
        "    st.sidebar.markdown(\"\"\"\n",
        "    1. Create a new Google Colab notebook\n",
        "    2. Install required packages:\n",
        "    ```python\n",
        "    !pip install streamlit pyngrok\n",
        "    ```\n",
        "    3. Copy this entire code into a cell\n",
        "    4. Save the code to a file:\n",
        "    ```python\n",
        "    with open('membrane_fouling_app.py', 'w') as f:\n",
        "        f.write('''\n",
        "    # Paste the entire code here\n",
        "    ''')\n",
        "    ```\n",
        "    5. Run the app with ngrok:\n",
        "    ```python\n",
        "    from pyngrok import ngrok\n",
        "    !streamlit run membrane_fouling_app.py --server.port=8501 &\n",
        "    public_url = ngrok.connect(port=8501)\n",
        "    print(f\"Streamlit app URL: {public_url}\")\n",
        "    ```\n",
        "    \"\"\")\n",
        "\n",
        "    st.sidebar.subheader(\"Local Streamlit Setup\")\n",
        "    st.sidebar.markdown(\"\"\"\n",
        "    1. Save the code to a file named `membrane_fouling_app.py`\n",
        "    2. Install required packages:\n",
        "    ```\n",
        "    pip install streamlit pandas numpy matplotlib seaborn scikit-learn\n",
        "    ```\n",
        "    3. Run the app:\n",
        "    ```\n",
        "    streamlit run membrane_fouling_app.py\n",
        "    ```\n",
        "    \"\"\")"
      ]
    }
  ]
}